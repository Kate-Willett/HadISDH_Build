Creation of HadISDH multivariate using both PHA and indirect PHA for 
T, Td, Tw, q, e, RH, DPD, WS and SLP

VERSION 4.2.0.2019f based on HadISD.3.1.0.2019f

Note: This is an annual update with a slightly different (and more) station make up to the previous
version (4.1.0.2018f) and may be deep past changes. There are now 8139 stations as opposed to 8103 stations (4.1.0.2018f) to begin selection from.
There are no significant code changes but some code has been converted to python 3 (from IDL) and some bugs have been found and corrected hence the increment to the Y.
The extra year may still change some internal statistics (neighbour correlations etc). The Z increment has been reset to 0 with 
the increment of Y.

# BUgs:
Incorrect reshaping of 20CR climatological array so climatological pressures used to obtain climatological station pressure was wrong. This had a very small affect on values.
RH when Tw <= 0 was being calculated relative to a wet bulb rather than an ice bulb
The monthly averaging was kicking out data with 15 years of observations within the climatology whereas we keep this data (>= 15 rather than >15)
The decade check for climatology was incorrect in IDL such that stations were passing through when they had no data in the 1981-1990 period.
MISSING DATA - April 2015 appears to be mostly missing (outside northwestern north america?). This may be a HadISD scrape issue. Will press on for now and
release a 4.2.1.2019f version later on in the year

Also note that the fixed bugs, particularly the one that changes the station make up I think, has a big enough effect to change the regional and global average
values. Only one piece of code has significantly changed (CreateMonthSeriesfromHadISD.py and I found this to product near identical values) and both IDL and python
MakeAreaAvgTS.py / make_area_avg_ts.pro produce very similar values.

I also discovered that the uncertainties pressented at the gridded level were not 1 sigma and inconsistent (2 sigma at station level, 4 sigma gridded station unc, 4 sigma combined)
so now these are all corrected to be 2 sigma.

We are now only running with the 1981-2010 climatology.

Other changes:
Having rewritten the sampling error code in python 3 last year I still can't use it until I have also rewritten the 
grid_HadISDHFLAT_JAN2015.pro code that calls it.
I have converted create_monthseriesJAN2015.pro to CreateMonthSeriesfromHadISD.py this year and found / corrected bugs in the old IDL code!!!

GLOSSARY:
PHA = Pairwise Homogenisation Algorithm v52j Menne and Williams, 2009
IDPHA = indirect PHA applied using changepoint locations from T/DPD and
adjustments derived from each variables own neighbour networks

;------------------------------------------------------------------
BUILD DIRECTORY STRUCTURE:
/data/users/hadkw/WORKING_HADISDH/UPDATE2019/ or $DATADIR/WORKING_HADISDH/UPDATE2019/

Copy the HadISD data from Robert Dunn (/scratch/rdunn/hadisd/v310_2019f/ to /data/users/hadkw/WORKING_HADISDH/HadISD/
This is BIG so you'll then need to gzip and delete as soon as it has been processed.
NOTE: HadISD now updates monthly. For consistency deep past updates are only carried out when the first month of the new year is
updated (e.g., v3.0.1_201901p) using the same station set up as last year but next year it will differ.

If we can use external harddrives with VDI?
Could make the HadISD.3.1.0.2019f directory in /media/Kate1Ext/ and move into it.
scp -c blowfish -r /scratch/rdunn/hadisd/v310_2019f/netcdf_files_v310_2019f/hadisd.3.1.0.2019f_*0.nc ., *1.nc...
Do this for *0.nc to *9.nc and check station counts. Don't copy internal, external, raw etc.
Get counts for each number by counting the _external.nc files - this is easier than trying to only count the number.nc files
ls hadisd.2.0.2.2017p_19310101-20171231_9*_external.nc.gz | wc -l
This year there are 8139 - 36 more than last year
Also copy the station lists:
cp /scratch/rdunn/hadisd/v310_2019f/hadobs_copy_v310_2019f/hadisd_station_fullinfo_v301_201901p.txt LISTS_DOCS/HadISD.3.1.0.2019f_candidate_stations_details.txt
cp /scratch/rdunn/hadisd/v310_2019f/input_files_v310_2019f/candidate_stations.txt LISTS_DOCS/HadISD.3.1.0.2019f_candidate_stations.txt
cp /scratch/rdunn/hadisd/v310_2019f/input_files_v310_2019f/final_mergers.txt LISTS_DOCS/HadISD.3.1.0.2019f_final_mergers.txt

NOTE: My external harddrive won't mount so I've used /data/users/hadkw/HadISD/ instead - hopefully this is temporary?

cp ../UPDATE2018/makeHadISDHdirectories.sh .

change years/versions in file where appropriate

./makeHadISDHdirectories.sh

	-> LISTS_DOCS
	  -> cp most recent isd-history.txt from ftp://ftp.ncdc.noaa.gov/pub/data/noaa to isd-history_downloadedDDJANYYYY_1230.txt
	  OR (this year the ftp is no longer working)
	  -> cp ../UPDATE2018/LISTS_DOCS/isd-history_downloaded18JAN2017_1230.txt LISTS_DOCS/
	-> IMAGES
		-> MAPS
		-> TIMESERIES
		-> BUILD
		-> ANALYSIS
		-> OTHER
	-> PROGS 
	(may need some faffing with the git repositories as described below but a straightforward copy appears to work and is in makeHadISDHdirectories.sh!
	This may (but didn't this time) then require all files to be added, committed and synced as they are seen as modifications:
	    -> git add filename
	    -> git commit -m "This is the 2017 update"
	    -> git push HADISDHOrigin master
	    -> git status THIS LETS YOU SEE WHAT NEEDS TO BE DONE)
	So the below stuff about git can be ignored but I'm keeping the info just in case    
	***
	 -> mkdir HADISDH_BUILD
	 -> cd HADISDH_BUILD
	 -> git init
	 -> git remote add HADISDHOrigin git@github.com:Kate-Willett/HadISDH_Build.git
	 -> git fetch HADISDHOrigin
	 -> git checkout master
	 -> cd ../
	 -> git init
	 -> git remote add ClimExpOrigin git@github.com:Kate-Willett/Climate_Explorer.git
	 -> git fetch ClimExpOrigin
	 -> git checkout master
	 ***
	 The PHA directories also appear to have copied across - so you'll need to empty them and reset to 2019
	 -> mkdir PHA2015
	 -> cd PHA2015
	 -> scp -r ../UPDATE2014/PHA_2014/.* .
	 change all years in pha52jgo/data/makePHAdirectories.sh then run.
	  -> ./makePHAdirecotories.sh
	  then remove the old .conf files and rm -R 73<yy><var> directories from hadisdh/
	  -> set up the .conf files, incl files as described in README_KATEJAN2016
	-> MONTHLIES 
		-> ASCII -> TABS,TDABS,TWABS,QABS,EABS,RHABS,DPDABS,WSABS,SLPABS
		            TANOMS,TDANOMS,TWANOMS,QABS,EANOMS,RHANOMS,DPDANOMS,
			    WSANOMS,SLPANOMS
			    raw monthly data
		-> HISTORY -> text files with dates of change: resolution,
		              frequency,merge
		-> NETCDF -> file for each station with all raw monthly 
		              variables
		-> HOMOG
			-> IDPHAASCII -> RHDIR,QDIR,EDIR,TWDIR,TDIR
			-> IDPHANETCDF -> RHDIR,QDIR,EDIR,TWDIR,TDIR
			-> PHAASCII -> DPDDIR,TDIR,TDDIR,WSDIR,SLPDIR (and 
			               others if necessary for comparison)
			-> PHANETCDF -> DPDDIR,TDIR,TDDIR, WSDIR, SLPDIR (and 
			               others if necessary for comparison)
	                -> STAT_PLOTS
				-> PHAADJCOMP -> DPDDIR,TDIR,TDDIR,WSDIR,SLPDIR
				                 (and others if necessary for 
						 comparison)
				-> IDADJCOMP -> RHDIR,QDIR,EDIR,TWDIR,TDIR
				-> UNCPLOTS -> RHDIR,QDIR,EDIR,TWDIR,TDIR,TDDIR,
				               DPDDIR, WSDIR,SLPDIR
				
	-> STATISTICS -> contains gridded product and derived statistics
		-> GRIDS
		-> TIMESERIES
		-> OTHER
	-> OTHERDATA
	  -> cp 20CRv2c... files from UPDATE2016/OTHERDATA/

;------------------------------------------------------------------
ORDER
1) SET UP PHA DIRECTORIES (mostly already done as described above)
        /data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/PROGS/PHA2015/
	- set up PHA directories:
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/PROGS/PHA2015/pha52jgo/data/
	-> makePHAdirectories.sh 
	- change all cases of 18 to 19 and then run ./makePHAdirectories.sh
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/PHA2015/pha52jgo/data/hadisdh/
							-> 73<yy><var>
								-> meta
								-> corr
								-> output
								-> monthly
									-> raw
									-> WMs.r00
									-> FLs.r00							
 	- set up /data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/PROGS/PHA2015/pha52jgo/data/73<yy><var>.conf
		- change datatag=7316... to datatag=7317...
		- change endyr=2016 to endyr=2017
		- hopefully maxyrs=300000 is ok but may need to be expanded now we're working with 8000+ stations

#OBSELETE - CONVERTED TO PYTHON 3 CODE 
#2) CREATE MONTHLY DATA FROM HOURLY HADISD
#/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/PROGS/HADISDH_BUILD/create_monthseriesJAN2015.pro	(all variables inclusive)
#	- change thisyear, nowmon, nowyear and version to appropriate values
#	- check all innie and outie filepaths to reflect date of run
#	-> make_months_oddtimesJAN2015.pro
#	-> calc_evap.pro
#	-> match.pro
#	Run this using a detached screen so that you can shutdown - it takes.....
#	navigate to PROGS/HADISDH_BUILD/
#	>screen
#	>tidl
#	>.compile match.pro
#	>.compile calc_evap.pro
#	>.compile make_months_oddtimesJAN2015.pro
#	>.compile create_monthseriesJAN2015.pro
#	>create_monthseriesJAN2015
#	to rerun after crash
#	>close,/all
#	>retall
#	>.compile filename
#	From another terminal
#	>screen -d
#	To re-enter
#	>screen -r
#	This can be restarted by changing newstart to the ID number
	
2) CREATE MONTHLY DATA FROM HOURLY HADISD
/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/PROGS/HADISDH_BUILD/CreateMonthSeriesfrom HadISD.py	(all variables inclusive)
	- change edyear,nowmon, nowyear and version to appropriate values
	- check all innie and outie filepaths to reflect 2017 run
	-> CalcHums.py
	-> ReadNetCDF.py
	-> MakeDayssince.py
	> module load scitools/default-current (python 3)
	> python CreateMonthSeriesfromHadISD.py
	This can be restarted by changing restarter to the 6 digit ID number
	This now outputs station list files ready for PHA rather than having to run the IDL code rewrite_stnlist.pro
	
RESULTS:
2019 from 8139 stations we now have 4497 'good' stations and 3642 'too short' stations 1981-2010 climatology
 - note that IDL gave us more stations than python because it had an error in the decade test for calculating climatology - so more were passed!
 - IDL did lose some stations because it required > 15 years on months for climatology (rather than >=)
 - INTERESTING that although we started with more stations this year we ended with fewer.
2018 From 8103 stations we now have 4572 'good' stations and 3531 'too short' stations.	1981-2010 climatology
2017 From 8103 stations we now have 4210 'good' stations and 3893 'too short' stations.	1976-2005 climatology
2017 From 8103 stations we now have 4576 'good' stations and 3527 'too short' stations.	1981-2010 climatology
2016 From 7877 stations we now have 4216 'good' stations and 3661 'too short' stations.	

3) RUN DIRECT PHA FOR T and DPD (and all others to get corr files): 
	- set up /data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/PROGS/PHA2015/pha52jgo/all_code/source_expand/parm_includes/inhomog.parm.MNTHLY.TEST.incl
	  end year, max number of stations (I just round up to nearest 100)
	- compile from /data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/PROGS/PHA2015/pha52jgo/all_code/
	>make compile -C source_expand
	OR
	- compile from /data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/PROGS/PHA2015/pha52jgo/all_code/source_expand
	>make compile
	- cp /data/users/hadkw/WORKING_HADISDH/UPDATE2017/PROGS/PHA2015/pha52jgo/all_code/source_expand/PHA*TEST 
             /data/users/hadkw/WORKING_HADISDH/UPDATE2017/PROGS/PHA2015/pha52jgo/code/bin/
#	- edit /PROGS/HADISDH_BUILD/rewrite_stnlist.pro to work with latest year and run to rewrite goodforHadISDH.<version>_JAN<yyyy>.txt
#	  station lists and populate PHA directories with station lists and blank metadata files: 
        - station lists are populated from CreateMonthSeriesfromHadISD.py now
	- edit /PROGS/HADISDH_BUILD/MakeMetaFilesPHA.sh to work with latest year and run to populate PHA directories with blank metadata files: 
		->/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/PROGS/PHA2015/pha52jgo/data/hadisdh/73<yy><var>/meta/
		->73<yy><var>_stnlist.tavg 
		->73<yy><var>_metadata_file.txt
	   
	- run from /data/users/hadkw/WORKING_HADISDH/UPDATE2017/PROGS/PHA2015/pha52jgo/
	nohup ./testv52i-pha.sh 73<yy><var> tavg raw 0 0 P > runlogs/73<yy><var>.log &
	
	OR (preferred for ease)
	- run from /data/users/hadkw/WORKING_HADISDH/UPDATE<YYYY>/PROGS/PHA2015/pha52jgo/
	- Check PROGS/PHA2015/pha52jgo/testv52j-phaSPICE.sh has the correct filepaths - with current year!!!
	- Update PROGS/PHA2015/pha52jgo/RunPHASPICE.sh has the updated years
	- For individual runs:
	>sbatch --mem=20000 --time=150 --ntasks=1 --output=runlogs/73<YY><var>.log ./testv52j-phaSPICE.sh 73<YY><var> tavg raw 0 0 P > runlogs/73<YY><var>.log &
	- For the whole lot:
	>./RunPHASPICE.sh

        OR
		
	- run from /scratch/hadkw:
	- Check PROGS/PHA2015/pha52jgo/testv52j-phaSPICE.sh has the correct filepaths
	- Update PROGS/PHA2015/pha52jgo/RunPHASPICE.sh has the updated years
	>scp -c blowfish -r /data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/PROGS/PHA2015/pha52jgo .(takes a while - 30-60mins?)
	- If you've just redone something then you can just rsync from /data/users/hadkw/WORKING_HADISDH/UPDATE2017/PROGS/PHA2015/pha52jgo/data/hadisdh/ 
	>rsync -r * /scratch/hadkw/pha52jgo/data/hadisdh/
	- For individual runs:
	>sbatch --mem=20000 --time=150 --ntasks=1 --output=runlogs/7317<var>.log ./testv52j-phaSPICE.sh 7317<var> tavg raw 0 0 P > runlogs/7317<var>.log &
	- For the whole lot:
	>./RunPHASPICE.sh	
	Copy the newly created files back to /data/local/ rsync from PROGS/PHA2015/pha52jgo/?
	>rync -r /scratch/hadkw/pha52jgo/* . takes AGES!!! 90mins?
	could just do this from data/hadisdh/ which might save a little time - ignore the runlogs

NOTE: 
May be some issue with length of filepath maxing out fortran line length. I had
problems with hadisdh7314dpd but not hadisdh7314t or hadisdh7314q. I had no
issues last year but the filepath was different/shorter. For this reason I now use 7316<var>

3a) Now run the python code RewriteStnlistPostPHA.py to make /LISTS_DOCS/goodforHadISDH.<version>_JAN<yyyy>.txt
        for each variable with the stations with fewer than 7 neighbours (corr > 0.1) removed, 
	goodforHadISDH.<version>_IDPHAall_JAN<yyyy>.txt with DPD and T few-neighbour stations removed, copy the 
	/corr/corr* files to /corr/corr.log and scrape the adjustment info from /output/PHA* into 
	HadISDH.land<var>.<version>_PHA_JAN<yyyy>.log which is also copied to /LISTS_DOCS/.
	This also creates 73<yy>td/corr/badlist.txt for PHAtd few neighbour stations for later use
	- Put the updated edyr, nowyear, version and PHAID in the Start section editables
	>python2.7 RewriteStnlistPostPHA.py
	- now copy the output on screen below and fill in the list of removed stations from /corr/meta*input_not_stnlist 

RESULTS:
BAD DPD and T uniq stations:  11
GOOD DPD and T uniq stations:  4486
0 DPD
BAD Stations for:  DPD 7
GOOD DPD uniq stations:  4490
68994099999 -46.88     37.87           22 SF MARION ISLAND                 
85470099999 -27.30    -70.41          298 CI CHAMONATE                     
85488099999 -29.92    -71.20          146 CI LA FLORIDA                    
89002099999 -70.67     -8.25           50 AY NEUMAYER                      
89022099999 -75.61    -26.27           30 AY HALLEY                        
91610099999   1.38    173.15            2 KR BONRIKI INTL                  
61998099999 -49.35     70.25           30 FS PORT-AUX-FRANCAIS (ILES KERGU 
1 T
BAD Stations for:  T 4
GOOD T uniq stations:  4493
8501099999  39.45    -31.13           34 PO FLORES                        
69146499999  40.87    141.38            5 JA DRAUGHON RANGE                
91066022701  28.20   -177.38            5 MQ HENDERSON FIELD AIRPORT       
91245041606  19.28    166.65            3 WQ WAKE ISLAND AIRFLD            
2 q
BAD Stations for:  q 4
GOOD q uniq stations:  4493
08501099999  39.45    -31.13           34 PO FLORES                        
69146499999  40.87    141.38            5 JA DRAUGHON RANGE                
85469099999 -27.16   -109.42           69 CI MATAVERI INTL                 
91066022701  28.20   -177.38            5 MQ HENDERSON FIELD AIRPORT     
3 e
BAD Stations for:  e 4
GOOD e uniq stations:  4493
08501099999  39.45    -31.13           34 PO FLORES                        
69146499999  40.87    141.38            5 JA DRAUGHON RANGE                
85469099999 -27.16   -109.42           69 CI MATAVERI INTL                 
91066022701  28.20   -177.38            5 MQ HENDERSON FIELD AIRPORT       
4 RH
BAD Stations for:  RH 10
GOOD RH uniq stations:  4487
61998099999 -49.35     70.25           30 FS PORT-AUX-FRANCAIS (ILES KERGU 
68994099999 -46.88     37.87           22 SF MARION ISLAND                 
85469099999 -27.16   -109.42           69 CI MATAVERI INTL                 
85470099999 -27.30    -70.41          298 CI CHAMONATE                     
85488099999 -29.92    -71.20          146 CI LA FLORIDA                    
89002099999 -70.67     -8.25           50 AY NEUMAYER                      
89022099999 -75.61    -26.27           30 AY HALLEY                        
89564099999 -67.60     62.87           16 AY MAWSON                        
91610099999   1.38    173.15            2 KR BONRIKI INTL                  
91958099999 -27.62   -144.33            2 FP RAPA            
5 Tw
BAD Stations for:  Tw 4
GOOD Tw uniq stations:  4493
08501099999  39.45    -31.13           34 PO FLORES                        
69146499999  40.87    141.38            5 JA DRAUGHON RANGE                
85469099999 -27.16   -109.42           69 CI MATAVERI INTL                 
91066022701  28.20   -177.38            5 MQ HENDERSON FIELD AIRPORT       
6 Td
BAD Stations for:  Td 5
GOOD Td uniq stations:  4492
08501099999  39.45    -31.13           34 PO FLORES                        
68906099999 -40.35     -9.88           54 SH GOUGH ISLAND                  
69146499999  40.87    141.38            5 JA DRAUGHON RANGE                
85469099999 -27.16   -109.42           69 CI MATAVERI INTL                 
91066022701  28.20   -177.38            5 MQ HENDERSON FIELD AIRPORT       

4) INFIL MDIs for MISSING YEARS, SAVE TO HOMOG ASCII DIRECTORY, PLOT RAW vs HOMOG with NEIGHBOURS, FOR T and DPD
OutputPHAASCIIPLOT_JAN2015.py
	THIS ONLY NEEDS TO BE DONE FOR T (PHA), DPD (PHA) and then after IDPHA for Td (PHADPD)
	FOR COMPARISONS IT CAN ALSO BE DONE FOR e, q, RH, Tw and Td (PHA)	

RESULTS:
DONE T
DONE DPD
DONE TD


DONE T
DONE DPD
after IndirectPHA_JAN2015.py for t
DONE Td PHADPD (the version which creates Td from T - DPD) 

Non-essentials:
	
5) RUN INDIRECT PHA FOR T (merge adjustment log), q, e, RH and Tw
IndirectPHA_JAN2015.py	(all variables seperately)
	-> LinearTrends.py
	- This program conducts IDPHA and outputs a list of adjustments and uncertainty and a new station list.
	- When run for T it also inputs a new station list for PHADPDtd which is then used by OutputPHAASCIIPLOT_JAN2015.py
	  This list has the Td no neighbours stations removed from it. It may be the same as T because the T noneighbours from
	  IDPHA tend to be those from Td that have not already been removed. This is a bit suspicious. I should check the
	  neighbour finding code. Why weren't these stations classed as having noneighbours from PHA? I think its because actually
	  PHA uses neighbours even if the correlation is less than 0.1 where as in IDPHA we say this is too low.
	- input station counts for q, e, Tw, RH, T and Td to plot_HadISDH_adjs_JAN2015.pro 

RESULTS:
DONE q
4481 stations in goods
5 no neighbours
61901099999-15.9330   -5.6670  436.0 SH ST. HELENA IS.                NGHBRS:    6 
85469099999-27.1650 -109.4220   69.2 CI MATAVERI INTL                 NGHBRS:    0 
89571099999-68.5830   77.9670   23.0 AY DAVIS                         NGHBRS:    5 
89642099999-66.6670  140.0170   43.0 AY DUMONT D'URVILLE              NGHBRS:    6 
91366040604  8.7330  167.7330    2.1 RM BUCHOLZ AAF KWAJALEIN KMR ATO NGHBRS:    6 


DONE RH
4478 stations in goods
8 no neighbours
61901099999-15.9330   -5.6670  436.0 SH ST. HELENA IS.                NGHBRS:    6 
71917099999 79.9830  -85.9330   10.0 CA EUREKA                        NGHBRS:    6 
84203099999 -2.1570  -79.8840    5.8 EC SIMON BOLIVAR INTL            NGHBRS:    6 
84452099999 -6.7870  -79.8280   29.6 PE CAPT JOSE A  QUINONES GONZALE NGHBRS:    6 
84691099999-13.7450  -76.2200   11.9 PE PISCO INTL                    NGHBRS:    6 
85469099999-27.1650 -109.4220   69.2 CI MATAVERI INTL                 NGHBRS:    0 
89564099999-67.6000   62.8670   16.0 AY MAWSON                        NGHBRS:    0 
91958099999-27.6170 -144.3330    2.0 FP RAPA                          NGHBRS:    0 
 
DONE T
4484 stations in goods
2 no neigbours
68906099999-40.3500   -9.8830   54.0 SH GOUGH ISLAND                  NGHBRS:    5 
85469099999-27.1650 -109.4220   69.2 CI MATAVERI INTL                 NGHBRS:    5 

DONE e
4481 stations in goods
5 no neighbouts
61901099999-15.9330   -5.6670  436.0 SH ST. HELENA IS.                NGHBRS:    6 
85469099999-27.1650 -109.4220   69.2 CI MATAVERI INTL                 NGHBRS:    0 
89571099999-68.5830   77.9670   23.0 AY DAVIS                         NGHBRS:    5 
89642099999-66.6670  140.0170   43.0 AY DUMONT D'URVILLE              NGHBRS:    6 
91366040604  8.7330  167.7330    2.1 RM BUCHOLZ AAF KWAJALEIN KMR ATO NGHBRS:    6 

DONE Tw
4483 stations in goods
3 no neighbours
08505099999 38.5200  -28.7160   36.0 PO HORTA                         NGHBRS:    6 
68906099999-40.3500   -9.8830   54.0 SH GOUGH ISLAND                  NGHBRS:    5 
85469099999-27.1650 -109.4220   69.2 CI MATAVERI INTL                 NGHBRS:    0 


6) DERIVE Td (and merge adjustment log), SAVE TO HOMOG ASCII DIRECTORY, PLOT RAW vs HOMOG with NEIGHBOURS, FOR Td
OutputPHAASCIIPLOT_JAN2014.py
	- this will need restarting whenever a Td station with no neighbours causes it to fail. Td doesn't need
	neighbours in this case - only for plotting. However, its easiest just to remove these stations from the list for 
	further processing
	NB: Future versions have an'IF NO NEIGHBOURS STILL OUTPUT FILE'
	
DONE Td

7) OBSELETE - I have automated this: Input station counts into plot_HadISDH_adjs_JAN2015.pro

8) CALCULATE MISSED ADJUSTMENT UNCERTAINTY AND PLOT ADJUSTMENT STATISTICS (Magnitude and time distributions)
plot_HadISDH_adjs_JAN2015.pro (all variables seperately called via command line)
        >tidl
	>.compile plot_HadISDH_adjs_JAN2015.pro
	>plot_HadISDH_adjs_JAN2015,'q','ID'   
	or 'dpd','rh','td','t','tw','e','q'
	or 'PHA' (dpd and all) or 'ID' (t, q, rh, e, tw) or 'DPD' (td)
	- Record missed adjustment uncertainty from plot and changepoint frequency/magnitude stats in program header
	- iterate to find histogram size that looks optimal - tweak 'pseudomax'
	- this programs finds all adjustments and lists them in Largest_Adjs_landq.4.0.0.2017f_IDPHA_JAN2018.txt to be
	used by UpdateGoodLists_LargeAdjRemovals_JAN2017.py
	- put missed adjustment uncertainties into create_homogNCDFall_stunc_JAN2016.pro
	- If you have time look in /LISTS_DOCS/HadISD.<version>_final_mergers.txt and look up large adjustment 
	(q>3, T/Td > 5, RH> 15) stations. Write MERGE or NO  MERGE next to that station in 
	LISTS_DOCS/Largest_Adjs_land<var>.<version>>_IDPHA_JAN<yyyy>.txt. Could also look to see whether it is a 
	'large' adjustment in T or Td and add IN T or IN TD as appropriate. Only really need to do this for key variables: T, Td, q and RH

NB - IF you rerun this you will have to run with _KeptLarge.txt!!!! You can switch between by editing
KeptLarge = 'TRUE' or 'FALSE' in the editable variables at the beginning of the code

NOTE: Not quite sure why the stations with largest adjs in q and e are not very large in either T or Td.

NOTE - NOW I'M REMOVING ALL STATIONS WHERE q>3 or RH>15 or T/TD > 5

RESULTS:
q,e,RH,Tw,T,Td - frequency of removals increases over time and is less in the early years - more station neighbours? 
DPD - goes up and then down

  q     ID    MN GSDFS:   -0.007
  q     ID     SDGSDFS:    0.221
  q     ID    MN ADJ N:    4.528
  e     ID    MN GSDFS:   -0.007
  e     ID     SDGSDFS:    0.272
  e     ID    MN ADJ N:    4.528
 rh     ID    MN GSDFS:   -0.132
 rh     ID     SDGSDFS:    1.160
 rh     ID    MN ADJ N:    4.530
  t     ID    MN GSDFS:    0.008
  t     ID     SDGSDFS:    0.299
  t     ID    MN ADJ N:    4.358
 tw     ID    MN GSDFS:   -0.012
 tw     ID     SDGSDFS:    0.233
 tw     ID    MN ADJ N:    4.528
 td    DPD    MN GSDFS:   -0.028
 td    DPD     SDGSDFS:    0.410
 td    DPD    MN ADJ N:    4.432
dpd    PHA    MN GSDFS:   -0.004
dpd    PHA     SDGSDFS:    0.251
dpd    PHA    MN ADJ N:    3.050

q removals > 3 = 6 stations
76577099999   3.99
76577099999   3.89
44347099999  -3.53
41128099999   3.52
16115099999   3.45
41240099999  -3.39
61498099999   3.29
41128099999  -3.14
44347099999   3.11

e removals from q (6 largest for e)
41240099999  -5.40
61498099999   5.21
76577099999   5.07
41128099999   4.96
76577099999   4.94
44347099999  -4.87

RH removals = 28 stations
44347099999 -37.65
36982099999 -26.38
04231099999  25.49
44347099999  24.64
44277099999 -22.46
71437099999  21.43
76577099999  20.17
71466099999  19.73
76577099999  19.72
72487099999 -19.66
01406099999 -18.92
37432099999 -18.63
71465099999  18.47
71800099999 -18.32
44298099999 -18.04
44284099999 -17.65
72487099999  17.52
47122099999  17.34
71261099999  17.28
71800099999  17.11
44287099999 -17.03
72355599999  17.01
59948099999  17.00
72570099999  16.65
44272099999  15.88
44347099999  15.77
38545099999 -15.76
72412799999  15.63
44213099999 -15.57
70333099999  15.55
60714099999 -15.19
85201099999  15.15
10935099999  15.13

Td removals = stations 31
71749099999  17.34
71749099999 -16.34
41128099999   9.47
44347099999  -8.63
71627899999   8.48
40841099999  -7.67
61498099999  -7.29
44277099999  -7.10
02095099999  -7.08
71627899999  -7.01
41128099999  -7.01
17330099999  -6.93
55664099999  -6.77
41128099999  -6.61
94332099999  -6.30
16115099999   6.24
41136099999  -6.20
72487099999   6.19
40420099999  -6.17
62103099999  -6.06
41128099999  -5.99
76471099999   5.94
41128099999   5.91
11993099999  -5.83
44284099999  -5.82
71836099999   5.81
94488099999  -5.79
40310099999  -5.69
72487099999  -5.66
40260099999   5.66
44358099999  -5.63
76382099999   5.55
71437099999   5.54
55279099999  -5.43
72365499999  -5.36
72365499999   5.31
44373099999   5.27
41136099999   5.22
41240099999  -5.21
44347099999   5.20
40340099999  -5.18
44347099999  -5.11
55578099999  -5.00

T removeals = 7 stations
71749099999 -17.79
71749099999  17.34
71627899999   9.31
71627899999  -8.85
02095099999  -7.08
71836099999   6.42
16115099999   6.24
11993099999  -5.83
71836099999  -5.20
53588099999   5.04

Tw removals = same stations as for T in top 15
71749099999  15.86
71749099999 -15.22
71627899999   8.58
02095099999  -6.49
71627899999  -6.46
71836099999   5.91
16115099999   5.54
16115099999  -5.27
71836099999  -4.39
53588099999   4.25
71836099999  -4.06
11993099999   3.84
71836099999  -3.84
71836099999   3.65
71836099999   3.65

OBSELETE 9) Manually save 'large' - > 5 deg adjustment stations from T and Td adjustments in a list of unique stations called:
HadISDH.3.0.0.2016p_LargeAdjT_Td_removals_JAN2017.txt in LISTS_DOCS

10) Run program to copy goodstations***(IDPHA, PHAdpd, PHADPDtd) to _KeptLarge.txt and remove 
all stations with T or Td adj > 5 deg, q > 3 g/kg or RH > 15% listed in Largest_Adjs_landT.<version>_IDPHAMG_JAN<yyyy>.txt
and Largest_Adjs_landTd.<version>_PHATd_JAN<yyyy>.txt and q and RH IDPHA equivalents
from each of the goodstations*** files (IDPHA, PHAdpd and PHADPDtd). 
	I can look at old time series of known issues here: /data/users/hadkw/WORKING_HADISDH/IMAGES/TESTCREATE
	Prog used: test_create_monthseries_MAY2012.pro, can be assessed using: check_HadCRUHstationsMAY2012.pro
	Red line = change of source
	Yellow line = change of frequency
	Blue line = change of resolution
UpdateGoodLists_LargeAdjRemovals_JAN2017.py

RESULTS 2019:
55 stations removed because of large adjustments - same as 2018	
Updated station counts are now:
    	IDPHA	PHA	PHADPD	MISSED ADJ UNC
T    	4429			0.299
DPD 		4435        	0.251
Td			4429    0.410
q	4426	                0.221
RH	4423	                1.160
e	4426	                0.272
Tw	4428	                0.233


RESULTS 2018:
55 stations removed because of large adjustments	
Updated station counts are now:
    	IDPHA	PHA	PHADPD	MISSED ADJ UNC
T    	4501			0.297
DPD 		4501        	0.270
Td			4501    0.406
q	4498	                0.216
RH	4497	                1.400
e	4498	                0.273
Tw	4501	                0.235

OBSELETE - now automated 11) Put the missed adjustment uncertainty (st dev of differences) into
create_homogNCDFall_stunc_JAN2015.pro
	
12) COPY HOMOGENISED MONTHLIES TO NETCDF - CREATE ANOMALIES, CLIMS, SDs, STATION UNCERTAINTIES
create_homogNCDFall_stunc_JAN2015.pro (all variables seperately)
	-> calc_evap.pro
	- update year, version, choose clim period etc
	- T first, then RH, DPD, q, e, td, tw
	- create_homogNCDFall_stunc_JAN2015,'t','ID'
	- Now cross check PosthomogPHA<var>_satsHadISDH<version>_JAN2017.txt and PosthomogPHA<var>_subzerosHadISDH<version>_JAN2017.txt with each other AND with
	Posthomog<ID>PHA<var>_badsHadISDH<version>_JAN2017.txt - move from sats/subzeros if station appears in bads and annotate bads with SATS or SUBS
	   This is quite a long process - consider automating somehow - better for avoiding error too!

RESULTS 2019 anoms81-10: 
IDPHA, PHDdpd, PHDDPDtd	
    	goods	bads	subs	sats 	goods (no sats or subs)
T    	4282	147	NA	NA	4282
RH	4398	25	0	65	4333 1 SATS IN BADS
DPD 	4092	343	NA	224     3868 17 SATS IN BADS	
q	4401	25	44	65	4332 NO SUBS/SATS IN BADS OR SUBS/SATS
e	4401	25	44	65	4332 NO SUBS/SATS IN BADS OR SUBS/SATS
Td	3921	508	NA	220  	3693 - no need to paste DPD removals in bads list as these are also in derived list
Tw	4403	25	NA	973	3430 NO SATS IN BADS


RESULTS 2018 anoms81-10:
IDPHA, PHDdpd, PHDDPDtd	
    	goods	bads	subs	sats 	goods (no sats or subs)
T    	4364	143	NA	NA	4364
DPD 	4090	419	NA	209     3881 16 SATS IN BADS REMOVED 	
Td	3914	587	NA	209  	3705 - no need to paste DPD removals in bads list as these are also in derived list
q	4481	17	38	41	4402 NO SUBS/SUBS OR SATS IN BADS
RH	4480	17	0	41	4439 NO SATS IN BADS
e	4481	17	39	41	4401 NO SATS/SUBS IN BADS
Tw	4484	17	NA	940	3544 NO SATS IN BADS

NOT SORTED FOR Td YET

RESULTS 2017 anoms81-10: 
IDPHA, PHDdpd, PHDDPDtd	
    	goods	bads	subs	sats 	goods (no sats or subs)
T    	4354	147	NA	NA	4364
DPD 	4077	437	NA	212     3865   	
Td	3928	579	NA	204  	3724 - no need to paste DPD removals in bads list as these are also in derived list
q	4485	17	44	48	4393
RH	4486	16	0	48	4438
e	4485	17	44	48	4393
Tw	4488	17	NA	896	3592

	
OBSELETE - NOW AUTOMATICALLY READ IN 13) Propogate totals of goods (no sats and subs), sats, subzeros through to all other programs

		
14) GRID ALL HOMOGENISED STATIONS (AND RAW FOR COMPARISON)
	- now made it all CF compliant and added an ascii grid print out
grid_HadISDHFLAT_JAN2015.pro
	-> calc_samplingerrorJUL2012_nofill.pro
	>tidl
	>.compile calc_samplingerrorJUL2012_nofill
	>.compile grid_HadISDHFLAT_JAN2015
	>grid_HadISDHFLAT_JAN2015,'q','ID'
	- 'q','e','rh','t','td','tw','dpd'
	- 'ID','ID','ID','ID','DPD','ID','PHA'

RESULTS: REDONE MARCH 2018 AFTER CORRECTING FILE_SEARCH bug
DONE q ID
DONE RH ID
DONE T ID
DONE e ID
DONE Tw ID
DONE DPD PHA

NOT DONE Td PHADPD

15) CREATE RAW AND HOMOGENISED DECADAL TREND GRIDBOX FIELDS 
#OLD:
#make_MP_trends.pro
#	-> median_pairwise.pro
#	You can do this for the full period, 1973-1999 and 2000-present.

NEW:
MakeGridTrends.py
  > module load scitools/default-current
  > python MakeGridTrends --var <var> --typee <type> --year1 <yyyy> --year2 <yyyy>
  var = 'dpd'	#'dpd','td','t','tw','e','q','rh'
  typee = 'PHA'	#'PHA','IDPHA','DPD', 'RAW','OTHER', 'BLEND', 'MARINE', 'BLENDSHIP', 'MARINESHIP'
  --year1 and --year2 are the start and end year of the trend period
  Under 'LONGER LIFE EDITABLES':
  - TrendType = 'OLS' for Ordinary Least Squares with AR(1) correction or TrendType = 'MP' for median pairwise
  - ConfIntP = 0.9 set to desired confidence interval 
  - MisingDataThresh = 0.7 set to desired minimum data presence over trend period
  - styr, edyr of dataset
  - climST, climED
  - nowmon,nowyear,thenmon,thenyear
  - version
  - domain = 'land', 'marine' or 'ocean'
THIS TAKES A LONG TIME TO RUN SO YOU CAN NOW RUN FROM SPICE
->./submit_spice_MAKEGridTrend.bash

RESULTS:
DONE q IDPHA
DONE RH IDPHA


16) CREATE RAW AND HOMOGENISED AREA AVERAGED TIME SERIES
# OLD:
#make_area_avg_ts.pro
#	-> globalmean.pro

# NEW:
MakeAreaAvgTS.py
  > module load scitools/default-current
  > python MakeAreaAvgTS # might later build command land choices in
  var = 'dpd'	#'dpd','td','t','tw','e','q','rh'
  typee = 'PHA'	#'PHA','IDPHA','DPD', 'RAW','OTHER', 'BLEND', 'MARINE', 'BLENDSHIP', 'MARINESHIP'
  Under 'LONGER LIFE EDITABLES':
  - TrendType = 'OLS' for Ordinary Least Squares with AR(1) correction or TrendType = 'MP' for median pairwise
  - ConfIntP = 0.9 set to desired confidence interval 
  - MisingDataThresh = 0.7 set to desired minimum data presence over trend period
  - styr, edyr of dataset
  - climST, climED
  - nowmon,nowyear,thenmon,thenyear
  - version
  - domain = 'land', 'marine' or 'ocean'
THIS NOW STOPS MID PROGRAM IF THERE IS A MISSING MONTH IN THE DATASET - YEARS STILL COMPLETED UNLESS MISSING DATA THRESHHOLD SET TO 1


RESULTS:
DONE q ID
DONE RH ID
DONE T ID
DONE e ID
DONE Tw ID
DONE Td PHADPD
DONE DPD PHA

17) PLOT RAW VS HOMOGENISED GRIDBOX AND AREA AVERAGE TREND STATS (2013 vs 2014 too)
plot_HadISDH_trendsscat_JAN2014.pro
	-> median_pairwise.pro
	-> plotsym.pro
	-> boxfill.pro
	-> make_key.pro

RESULTS:
DONE q ID
DONE RH ID
DONE T ID (had to expand ymax to 1.2)
DONE e ID
DONE Tw ID
DONE Td PHADPD
DONE DPD PHA

18) PLOT DECADAL TRENDS FOR THE GRIDBOX
plot_HadISDH_MPdectrends_JAN2014.pro (/data/users/hadkw/WORKING_HADISDH/UPDATE<YYYY>/PROGS/IDL/)
	-> boxfill.pro
	-> make_key.pro
Can also use PlotTrendMap_JAN2015.py in UPDATE<YYYY>/PROGS/PYTHON/	***PREFERRED***


19) PLOT ANNUAL ANOMALY MAPS FOR HADOBS
/data/users/hadkw/WORKING_HADISDH/UPDATE<YYYY>/PROGS/PYTHON/
PlotAnyMap_JUN2015.py
        Update year, version, ThenYrMon
	Choose Domain, Var and any other specific plot settings
	> module load scitools/default-current (python 3)
	> python PlotAnyMap_JUN2015.py

# NOW REDUNDANT AS WE HAVE A PYTHON VERSION
#/data/users/hadkw/WORKING_HADISDH/UPDATE2016/PROGS/IDL/
#run_annualanommaps.pro
#plot_annualanommaps_FEB2015.pro
#	-> boxfill.pro
#	-> make_key.pro
#	need to update:
#		plot_annualanomaps_FEB2015.pro
#		- edyr
#		- ensure varid points to q_anoms, rh_anoms et
#		run_annualanomaps.pro
#		- version, nowmon, nowyear, thenmon,thenyear, homogtype, param 
#		- ensure filepaths and pointers are up to date
#	Make directories called ANOMS7605_7605 to reflect the use of anoms7605 for producing anomalies relative to 7605 (can choose a different period)
#	Move all maps (images and data grids) into these directories for tidiness
	
20) Create the CEDA versions
/data/users/hadkw/WORKING_HADISDH/UPDATE2016/PRnedtOGS/PYTHON/
WriteNetCDF_CEDAESGF_JAN2016.py
Convert_CEDAESGF_JAN2016.py
	- Follow ~hadkw/Desktop/HadISDH/CEDA_DIR/UPDATING_CEDA_README/ to update the CEDA versions
	including documentation
	
RESULTS:
DONE ESGF
DONE Copy ASCII
DONE check table - same
DONE write update doc	

21) Build uncertainties for area averages
    
    This first requires download and update of ERA5 q and RH etc
    cp UPDATE<prevyear>/OTHERDATA/197901_hourly_land_sea_mask.nc the UPDATE<YYYY>/OTHERDATA/ (or run submit_spice_ERA5download_lsm.bash (get_era5_lsm.py) - check directories!!!)
    cp UPDATE<prevyear>/OTHERDATA/<var>2m_<time>_<res>_ERA5_1979<prevyear>.nc to the UPDATE<YYYY>/OTHERDATA/
    download the hourly 0.25by0.25 files in YYYYMM chunks for T, Td and P and save as daily 1by1 YYYYMM chunks for all humidity variables
    ->PROGS/PYTHON/submit_spice_ERA5download.bash
    ->PROGS/PYTHON/get_era5.py
    	- update the years to download and check directory paths in get_era5.py
        >module load scitools/default-current (seem to have to do this first even if running on spice)
	>./submit_spice_ERA5download.bash
	This may need restarting as it falls over often - and takes ages
	At the moment I have set up OTHERDATA/ERA5/ with all daily data. I hope that we can just download the latest year, make into pentads/months, append to previous year's actual
	values then recalculate anomalies etc.
    now make pentads, months and 5by5 grids from the dailies
    ->PROGS/PYTHON/MergeAggRegridERA5.py
    ->PROGS/PYTHON/spice_MergeERA.bash
    	- edit directory paths for this year's update
	- choose variable and time resolution (pentad, monthly) in the spice_MergeERA5.py file (could write a submit_spice_MergeERA5.bash code!)
        >module load scitools/default-current (seem to have to do this first even if running on spice)
	>sbatch spice_MergeERA5.py for specific set ups
	>./submit_spice_MergeERA5.py to run some/all variables for pentad and monthly
	THIS DOESN'T TAKE TOO LONG NOW AND SHOULD BE VERY FAST FOR UPDATES
    Now run the uncertainties
    ->PROGS/HADISDH_BUILD/hadisdh_error_calculations.py to create regional average timeseries and uncertainties 
    Edit filepath, version and date at top of file - and domain
    Edit filenames within the file to make sure they are the latest version
    >module load scitools/default-current
    >python2 hadisdh_error_calculations.py    
    THESE ARE ALL 2 SIGMA!!!
    
22) Follow instructions on ~hadkw/Desktop/HadISDH/CEDA_DIR/UPDATING_CEDA_README to do the CEDA update and also create the update document


23) Put up new version on HadOBS (www.metoffice.gov.uk/hadobs/hadisdh)
SEE /data/users/hadkw/HADOBS_HADISDH/README_HADOBSUPDATE
	- login as hadobs: ssh -Y hadobs@vld098 or ssh -Y hadobs@eldint01
	- >bash to get nice unix features
	- navigate to /project/hadobs1/OBS/www.hadobs.org/hadisdh/
	  - mkdir <oldversion>
	  - cp the anomalymapmaterial_<var>.html files, HadISDH.<version>_update.pdf (move!), download<xyz>.html, index.html and onlinematerial<xyz>.html into the old version
	    directory
	  - copy ~hadkw/Desktop/HadISDH/CEDA_DIR/HadISDH.v4.0.0.2017f_update.pdf to main
	  - remove the older version directory and contents - this will now only be available by email.
	- go into /data/
	  - mkdir v3002016p
	  - move all .txt, .dat and .nc files for that version into the old version directory and gzip
	  - copy new Posthomog station lists, netcdf and ascii grids and full station list to main data/
	  - tar ball the homogenised netCDF station files for each variable to HadISDH.land<var>.<version>.stations (tar -czf)
	  - cp the tarball of stations to the main data/
	  - remove the older version and contents - this will now only be available by email
	- go into /images/
	  - clear out everything in olderversions
	  - move all HadISDH.land* plots from latest_ersion to olderversions and gzip
	  - copy HadISDH.land*annualanom8110*png maps to latest_version
	  - copy PlotRegions*png and PlotGlobTimeseries*png time series to latest_version
	- open index.html and update
	- set up new version pages
	go into scripts and type: >export_hadisdh to upload to the hadley server 
	NOTE: If you need to log on to the hadley server:
	- >ssh -Y hadobs@eld256
	- >bash
	- >ssh hadobs@hadsrv01-zvedge
	- >cd /project/local/www/html/hadobs/hadisdh/

DONE

;------------------------------------------------------------------
DETAILS
;******************************************************************
create_monthseriesJAN2015.pro
Take the updated (QC'd) HadISD hourly T and Td data and calculate hourly: q, e, 
RH, Tw and DPD. Average to monthly using make_months_oddtimesJan2014.pro. Derive
monthly Td and DPD. Create HISTORY files with dates of changes in resolution, 
reporting frequency or merging. Save to netCDF and ASCII and also as a .raw in 
PHA2015.

	Inputs:
	/media/Kate1Ext3//HadISD.2.0.1.2016p/'		; QC'd HadISD stations
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/HadISD.2.0.2.2017p_candidate_stations_details.txt'	; full 6103 station list
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/isd-history_downloaded23JAN2017_1230.txt'	; most recent ISD station list with CIDs 
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/OTHERDATA/'		;20CR SLP 20CR*7605MSLP_yycompos.151.170.240.10.37.8.8.59.nc

	Outputs:
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/MONTHLIES/ASCII/'	; directoriies for ASCII
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/PROGS/PHA2015/pha52jgo/data/hadisdh/73<yy>q/monthly/raw/'   ; directories for PHA
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/PROGS/PHA2015/pha52jgo/data/hadisdh/73<yy>e/monthly/raw/'
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/PROGS/PHA2015/pha52jgo/data/hadisdh/73<yy>t/monthly/raw/'
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/PROGS/PHA2015/pha52jgo/data/hadisdh/73<yy>td/monthly/raw/'
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/PROGS/PHA2015/pha52jgo/data/hadisdh/73<yy>tw/monthly/raw/'
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/PROGS/PHA2015/pha52jgo/data/hadisdh/73<yy>rh/monthly/raw/'
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/PROGS/PHA2015/pha52jgo/data/hadisdh/73<yy>slp/monthly/raw/'
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/PROGS/PHA2015/pha52jgo/data/hadisdh/73<yy>ws/monthly/raw/'
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/MONTHLIES/HISTORY/'	; directory for history files
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/MONTHLIES/NETCDF/'	; directory for NetCDF files
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/tooshortforHadISDH.<version>_JAN2017.txt'	; list of removed stations
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/goodforHadISDH.<version>_JAN2017.txt'		; list of kept stations

;******************************************************************
run direct PHA FOR T and DPD : /data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/PROGS/PHA2015/pha52jgo/
	INPUTS:
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/PROGS/PHA2015/pha52jgo/data/hadisdh/73<yy><var>/monthly/raw:
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/PROGS/PHA2015/pha52jgo/data/hadisdh/73<yy><var>/meta:

	OUTPUTS:
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/PROGS/PHA2015/pha52jgo/runlogs/73<yy><var>.log
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/PROGS/PHA2015/pha52jgo/data/hadisdh/73<yy><var>/corr/corr.73<yy><var>.tavg.r00.<timedate>
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/PROGS/PHA2015/pha52jgo/data/hadisdh/73<yy><var>/corr/meta.73<yy><var>.tavg.r00.<timedate>.1.input_not_stnlist
	(may be more than 1)
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/PROGS/PHA2015/pha52jgo/data/hadisdh/73<yy><var>/outputs/*
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/PROGS/PHA2015/pha52jgo/data/hadisdh/73<yy><var>/monthly/WMs.r00/*
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/PROGS/PHA2015/pha52jgo/data/hadisdh/73<yy><var>/monthly/FLs.r00/*
	Station counts for DPD

;*****************************************************************
RewriteStnlistPostPHA.py
       
	INPUTS:
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/goodforHadISDH.<version>_JAN<yyyy>.txt
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/PROGS/PHA2015/pha52jgo/data/hadisdh/73<yy><var>/corr/corr.73<yy><var>.tavg.r00.<timedate>
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/PROGS/PHA2015/pha52jgo/data/hadisdh/73<yy><var>/corr/meta.73<yy><var>.tavg.r00.<timedate>.1.input_not_stnlist
	(may be more than 1)
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/PROGS/PHA2015/pha52jgo/data/hadisdh/73<yy><var>/outputs/PHA*	

	OUTPUTS:
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/goodforHadISDH.<version>_PHA<var>_JAN<yyyy>.txt
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/HadISDH.land<var>.<version>_PHA_JAN<yyyy>.log
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/goodforHadISDH.<version>_IDPHAall_JAN<yyyy>.txt
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/PROGS/PHA2015/pha52jgo/data/hadisdh/73<yy><var>/corr/corr.log
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/PROGS/PHA2015/pha52jgo/data/hadisdh/73<yy>td/corr/badlist.txt
	
;*******************************************************************	
OutputPHAASCIIPLOT_JAN2014.py
	INPUTS:
        /data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/goodforHadISDH.<version>_PHA<var>_JAN2017.txt  	; station list
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/PROGS/PHA2015/pha52jgo/data/hadisdh/73<yy><var>/monthly/WMs.r00/*	; homogenised data
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/MONTHLIES/ASCII/<VAR>ABS/						; raw data
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/PROGS/PHA2015/pha52jgo/data/hadisdh/73<yy><var>/corr/corr.73<yy><var>.tavg.r00.<timedate> ; neighbour network list	
	Td only:
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/HadISDH.landDPD.'+version+'_JAN2017.log		; DPD adjustment log
        /data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/HadISDH.landT.'+version+'_IDPHA_JAN2017.log		; T adjustment log
	
	OUTPUTS:
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/MONTHLIES/HOMOG/PHAASCII/<VAR>DIR/*_PHAadj.txt	; ASCII homogenised data
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/MONTHLIES/HOMOG/STAT_PLOTS/PHAADJCOMP/<VAR>DIR/*	; raw vs homogenised and neighbours time series plot
	Td only:
        /data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/HadISDH.landTd.'+version+'_DPDPHA_JAN2017.log		; Td merged adjustment log
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/MONTHLIES/HOMOG/IDPHAASCII/<VAR>DIR/*_PHAadj.txt	; ASCII homogenised data
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/MONTHLIES/HOMOG/STAT_PLOTS/IDADJCOMP/<VAR>DIR/*	; raw vs homogenised and neighbours time series plot

;*******************************************************************       
IndirectPHA_JAN2014.py
	INPUTS:
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/goodforHadISDH.<version>_IDPHAall_JAN2017.txt	;station list for T and DPD combined
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/HadISDH.landT.<version>_PHA_JAN2017.log	; adjustment logs
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/HadISDH.landDPD.<version>_PHA_JAN2017.log	; adjustment logs
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/PROGS/PHA2015/pha52jgo/data/hadisdh/73<yy><var>/corr/corr.73<yy><var>.tavg.r00.<timedate>	; neighbour network list
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/MONTHLIES/ASCII/<VAR>ABS/					; raw data
	T only:
        /data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/HadISDH.landT.'+version+'_IDPHA_JAN2017.log		; adjustment log	
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/PROGS/PHA2015/pha52jgo/data/hadisdh/73<yy>td/corr/badlist.txt
	
	OUTPUTS:
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/goodforHadISDH.<version>_ID<var>_JAN2017.txt	;station list for T and DPD combined
        /data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/HadISDH.land<var>.'+version+'_<homogtype>PHA_JAN2017.log		; adjustment log
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/MONTHLIES/HOMOG/IDPHAASCII/<VAR>DIR/*_<homogtype>PHAadj.txt	; ASCII homogenised data
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/MONTHLIES/HOMOG/STAT_PLOTS/IDADJCOMP/<VAR>DIR/*	; raw vs homogenised and neighbours time series plot
	T only:
        /data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/HadISDH.landT.'+version+'_IDPHAMERGE_JAN2017.log		; adjustment log
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/goodforHadISDH.<version>_PHADPDtd_JAN2017.txt	;station list for T and DPD combined
	Station counts for T (and Td), Tw, q, e, RH
	
;*******************************************************************
plot_HadISDH_adjs_JAN2014.pro
      	INPUTS:
      	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/goodforHadISDH.'+version+'_<homogtype>PHA<var>_JAN2017.txt'
      	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/HadISDH.landq.'+version+'_<homogtype>PHA_JAN2017.log' 
      	OUTPUTS:
      	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/IMAGES/HadISDH.landq.'+version+'_adjspread_<homogtype>PHA_'+nowmon+nowyear+'.eps'
      	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/Largest_Adjs_land<var>.'+version+'_<homogtype>PHA_'+nowmon+nowyear+'.txt'
      	missed adjustment uncertainty value (st dev of estimated complete adjustment distribution minus actual distribution)
	A MANUAL list of T and Td stations with adjustments larger than 5 degrees: HadISDH.3.0.0.2016p_LargeAdjT_Td_removals_JAN2017.txt

;********************************************************************
UpdateGoodLists_LargeAdjRemovals_JAN2017.py
	INPUTS:
        /data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/HadISDH.3.0.0.2016p_LargeAdjT_Td_removals_JAN2017.txt
        /data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/goodforHadISDH.3.0.0.2016p_IDPHAt_JAN2017.txt
        /data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/goodforHadISDH.3.0.0.2016p_IDPHAq_JAN2017.txt
        /data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/goodforHadISDH.3.0.0.2016p_IDPHArh_JAN2017.txt
        /data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/goodforHadISDH.3.0.0.2016p_IDPHAe_JAN2017.txt
        /data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/goodforHadISDH.3.0.0.2016p_IDPHAtw_JAN2017.txt
        /data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/goodforHadISDH.3.0.0.2016p_PHADPDtd_JAN2017.txt
        /data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/goodforHadISDH.3.0.0.2016p_PHAdpd_JAN2017.txt
	OUTPUTS:
        /data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/goodforHadISDH.3.0.0.2016p_IDPHAt_JAN2017.txt
        /data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/goodforHadISDH.3.0.0.2016p_IDPHAq_JAN2017.txt
        /data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/goodforHadISDH.3.0.0.2016p_IDPHArh_JAN2017.txt
        /data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/goodforHadISDH.3.0.0.2016p_IDPHAe_JAN2017.txt
        /data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/goodforHadISDH.3.0.0.2016p_IDPHAtw_JAN2017.txt
        /data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/goodforHadISDH.3.0.0.2016p_PHADPDtd_JAN2017.txt
        /data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/goodforHadISDH.3.0.0.2016p_PHAdpd_JAN2017.txt
        /data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/goodforHadISDH.3.0.0.2016p_IDPHAt_JAN2017_KeptLarge.txt
        /data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/goodforHadISDH.3.0.0.2016p_IDPHAq_JAN2017_KeptLarge.txt
        /data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/goodforHadISDH.3.0.0.2016p_IDPHArh_JAN2017_KeptLarge.txt
        /data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/goodforHadISDH.3.0.0.2016p_IDPHAe_JAN2017_KeptLarge.txt
        /data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/goodforHadISDH.3.0.0.2016p_IDPHAtw_JAN2017_KeptLarge.txt
        /data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/goodforHadISDH.3.0.0.2016p_PHADPDtd_JAN2017_KeptLarge.txt
        /data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/goodforHadISDH.3.0.0.2016p_PHAdpd_JAN2017_KeptLarge.txt


;********************************************************************
create_homogNCDFall_stunc_JAN2015.pro
    	INPUTS:
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/goodforHadISDH.'+version+'_<homogtype>PHA<var>_JAN2014.txt
    	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/MONTHLIES/HOMOG/IDPHAASCII/<VAR>DIR/*_PHAadj.txt
       	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/MONTHLIES/HOMOG/IDPHANETCDF/RHDIR/ saturation test and uncertainty bins
       	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/MONTHLIES/HOMOG/IDPHANETCDF/TDIR/ saturation test and uncertainty bins
    	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/HadISDH.landq.'+version+'_IDPHA_JAN2014.log'     ;***
    	OUTPUTS:
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/Posthomog<homogtype>PHA<var>_anoms7605_goodsHadISDH.'+version+'_'+nowmon+nowyear+'.txt'
     	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/Posthomog<homogtype>PHA<var>_anoms7605_satsHadISDH.'+version+'_'+nowmon+nowyear+'.txt'
    	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/Posthomog<homogtype>PHA<var>_anoms7605_badsHadISDH.'+version+'_'+nowmon+nowyear+'.txt'
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/Posthomog<homogtype>PHA<var>_anoms7605_subzerosHadISDH.'+version+'_'+nowmon+nowyear+'.txt'
        /data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/MONTHLIES/HOMOG/IDPHANETCDF/<VAR>DIR/ *anoms7605_*
        /data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/MONTHLIES/HOMOG/STAT_PLOTS/UNCPLOTS/<VAR>DIR/' *anoms7605_*

;*******************************************************************
grid_HadISDHFLAT_JAN2015.pro
      	INPUTS:
      	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/Posthomog<homogtype>PHA<var>_anoms7605_goodsHadISDH.'+version+'_JAN2017.txt
      	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/Posthomog<homogtype>PHA<var>_anoms7605_satsHadISDH.'+version+'_JAN2017.txt
      	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/Posthomog<homogtype>PHA<var>_anoms7605_subsHadISDH.'+version+'_JAN2017.txt
      	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/MONTHLIES/HOMOG/IDPHANETCDF/<VAR>DIR/ *anoms7605_*
      	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/MONTHLIES/NETCDF/	; raw data
      	OUTPUTS:
      	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/STATISTICS/HadISDH.land<var>.'+version+'_FLATgrid<homogtype>PHA5by5_anoms7605_JAN2017.nc 
      	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/LISTS_DOCS/GriddingResults_3.0.0.2016p_anoms7605_JAN2017.txt	max/mins of all fields in nc file 

;*******************************************************************
make_MP_trends.pro
	INPUTS:
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/STATISTICS/HadISDH.land<Var>.'+version+'_FLATgrid<homogtype>PHA5by5_anoms7605_JAN2017
	OUTPUTS:
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/STATISTICS/HadISDH.land<Var>.'+version+'_FLATgrid<homogtype>PHA5by5_JAN2017_anoms7605_MPtrends_19732016.nc
		
;*******************************************************************
make_area_avg_ts.pro
	INPUTS:
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/STATISTICS/HadISDH.land<Var>.'+version+'_FLATgrid<homogtype>PHA5by5_anoms7605_JAN2014
	OUTPUTS:
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/STATISTICS/HadISDH.land<Var>.'+version+'_FLATgrid<homogtype>PHA5by5_JAN2014_anoms7605_areaTS_19732013.nc

;*******************************************************************
plot_HadISDH_MPtrendsscat_JAN2014.pro
	INPUTS:
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/STATISTICS/HadISDH.land<Var>.'+version+'_FLATgrid<homogtype>PHA5by5_JAN2014_anoms7605_MPtrends_19732013.nc
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/STATISTICS/HadISDH.land<Var>.'+version+'_FLATgrid<homogtype>PHA5by5_JAN2014_anoms7605_areaTS_19732013.nc
	OUTPUTS:
  	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/IMAGES/HadISDH.land<Var>.'+version+'_FLATgrid<homogtype>PHA5by5_JAN2014_anoms7605_MPtrendsscat_19732013.eps

;*******************************************************************
plot_HadISDH_MPdectrends_JAN2014.pro
	INPUTS:
	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/STATISTICS/HadISDH.land<Var>.'+version+'_FLATgrid<homogtype>PHA5by5_JAN2014_anoms7605_MPtrends_19732013.nc
	OUTPUTS:
  	/data/users/hadkw/WORKING_HADISDH/UPDATE<yyyy>/IMAGES/HadISDH.land<Var>.'+version+'_FLATgrid<homogtype>PHA5by5_JAN2014_anoms7605_MPdectrends_19732013.eps

;*******************************************************************
BAD STATIONS???
041150 - has two periods of data with a large gap. The earlier period is quite intermittent an has a climatology closer to -30 (T and Td). The second has a climatology closer to 0 (T and Td). Does
not appear to be a station merge. Could be an error in early period (divide by 10?) or in later period (* 10?). Its at 65N so not super high latitude.
TOO SHORT FOR 1981-2010 CLIM SO NOT USED

;*******************************************************************
NOTE TO KATE:
Some minimum threshold for uncertainties? In some cases measurement uncertainty is zero.

2014 run:
Modifiy make_month_oddtimes to mask abs to anoms or actually create abs from the anoms+clims to reduce biasing
where data are unevenly distributed.

